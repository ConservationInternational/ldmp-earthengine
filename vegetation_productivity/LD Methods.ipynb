{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the Earth Engine Python Package\n",
    "import ee\n",
    "import sys\n",
    "import json\n",
    "from landdegradation import stats\n",
    "from landdegradation import util\n",
    "\n",
    "# Initialize the Earth Engine object, using the authentication credentials.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# USER INPUT for testing, REMOVE LATER\n",
    "year_start = 2001\n",
    "year_start_str = '2001'\n",
    "year_end   = 2015\n",
    "year_end_str = '2015'\n",
    "sa_name    = 'KEN'      # 'globe' or ISO3 code\n",
    "method     = 'ndvi_trend' # 'ndvi_trend','prestrend', 'rue_trend', wue_trend'\n",
    "sensor     = 'modis'      # 'modis' or 'avhrr'\n",
    "climate    = 'prec_gpcp'  # 'prec_gpcp', 'prec_persian', 'prec_chirps', 'prec_gpcc', 'soilm_merra2', 'soilm_erai', 'et_mod16a2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# INPUT DATA\n",
    "# Country boundaries\n",
    "if (sa_name == 'globe'):\n",
    "    sa = ee.Geometry.Polygon([-180, 80, 0, 80, 180, 80, 180, -80, 0, -80, -180, -80])\n",
    "elif (sa_name != 'globe'):\n",
    "    sa = ee.FeatureCollection(\"USDOS/LSIB/2013\").filter(ee.Filter.eq('iso_alpha3', sa_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sensors        \n",
    "if (sensor == 'avhrr'):\n",
    "    ndvi_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/ndvi_avhrr_1982_2015\")\n",
    "    pixel = 8000\n",
    "elif (sensor == 'modis'):\n",
    "    ndvi_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/ndvi_modis_2001_2016\")\n",
    "    pixel = 250\n",
    "else:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Climate\n",
    "if (climate == 'et_mod16a2'):\n",
    "    climate_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/et_modis_2000_2014\")\n",
    "if (climate == 'prec_gpcp'):\n",
    "    climate_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/prec_gpcp23_1979_2016\")\n",
    "if (climate == 'prec_gpcc'):\n",
    "    climate_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/prec_gpcc_1901_2016\")\n",
    "if (climate == 'prec_chirps'):\n",
    "    climate_1yr =  ee.Image(\"users/geflanddegradation/toolbox_datasets/prec_chirps_1981_2016\")\n",
    "if (climate == 'prec_persian'):\n",
    "    climate_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/prec_persian_1983_2015\")\n",
    "if (climate == 'soilm_merra2'):\n",
    "    climate_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/soilm_merra2_1980_2016\")\n",
    "if (climate == 'soilm_areai'):\n",
    "    climate_1yr = ee.Image(\"users/geflanddegradation/toolbox_datasets/soilm_erai_1979_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Land cover\n",
    "landc = ee.Image(\"users/geflanddegradation/toolbox_datasets/lcov_esacc_1992_2015\").select('y'+year_end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define Kendall parameter values for a significance of 0.05\n",
    "period = year_end - year_start + 1\n",
    "coefficients = ee.Array([4, 6, 9, 11, 14, 16, 19, 21, 24, 26, 31, 33, 36,\n",
    "                        40, 43, 47, 50, 54, 59, 63, 66, 70, 75, 79, 84,\n",
    "                        88, 93, 97, 102, 106, 111, 115, 120, 126, 131,\n",
    "                        137, 142])\n",
    "kendall = coefficients.get([period - 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to calc ndvi trend\n",
    "\n",
    "if (method == 'ndvi_trend'):\n",
    "    def f_img_coll(ndvi_stack):\n",
    "        img_coll = ee.List([])\n",
    "        for k in range(year_start, year_end):\n",
    "            ndvi_img = ndvi_stack.select('y' + str(k)).addBands(ee.Image(k).float()).rename(['ndvi', 'year'])\n",
    "            img_coll = img_coll.add(ndvi_img)\n",
    "        return ee.ImageCollection(img_coll)\n",
    "    \n",
    "    ## Apply function to compute NDVI annual integrals from 15d observed NDVI data\n",
    "    ndvi_1yr_coll = f_img_coll(ndvi_1yr)\n",
    "    \n",
    "    ## Compute linear trend function to predict ndvi based on year (ndvi trend)\n",
    "    lf_trend = ndvi_1yr_coll.select(['year', 'ndvi']).reduce(ee.Reducer.linearFit())\n",
    "\n",
    "    ## Compute Kendall statistics\n",
    "    mk_trend  = stats.mann_kendall(ndvi_1yr_coll.select('ndvi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Function to create image collection with annual NDVI integrals and years\n",
    "\"\"\"\n",
    "### translated direct\n",
    "if (method == 'prestrend'):\n",
    "    def f_img_coll(ndvi_stack):\n",
    "        img_coll = ee.List([])\n",
    "        for (k = year_start k <= year_end  k += 1):\n",
    "        ndvi_img = ndvi_stack.select('y'+k)\n",
    "                                .addBands(climate_1yr.select('y'+k))\n",
    "                                .rename(['ndvi','clim']).set({'year': k})    \n",
    "        img_coll = img_coll.add(ndvi_img)\n",
    "    return ee.ImageCollection(img_coll)  \n",
    "\"\"\"\n",
    "if (method == 'prestrend'):\n",
    "    def f_img_coll(ndvi_stack):\n",
    "        img_coll = ee.List([])\n",
    "        for k in range(year_start, year_end):\n",
    "            ndvi = ndvi_1yr.filter(ee.Filter.eq('year', k)).select('ndvi').median()\n",
    "            clim = climate_1yr.filter(ee.Filter.eq('year', k)).select('ndvi').median()\n",
    "            img = ndvi.addBands(clim.addBands(ee.Image(k).float())).rename(['ndvi','clim','year']).set({'year': k})\n",
    "            img_coll = img_coll.add(img)\n",
    "            return ee.ImageCollection(img_coll)\n",
    "\n",
    "    ## Function to predict NDVI from climate\n",
    "    first = ee.List([])\n",
    "    def f_ndvi_clim_p(image, list):\n",
    "        ndvi = lf_clim_ndvi.select('offset').add((lf_clim_ndvi.select('scale').multiply(image))).set({'year': image.get('year')})\n",
    "        return ee.List(list).add(ndvi)\n",
    "\n",
    "    ## Function to compute residuals (ndvi obs - ndvi pred)\n",
    "    def f_ndvi_clim_r_img(year): \n",
    "        ndvi_o = ndvi_1yr_coll.filter(ee.Filter.eq('year', year)).select('ndvi').median()\n",
    "        ndvi_p = ndvi_1yr_p.filter(ee.Filter.eq('year', year)).median()\n",
    "        ndvi_r = ee.Image(year).float().addBands(ndvi_o.subtract(ndvi_p))\n",
    "        return ndvi_r.rename(['year','ndvi_res'])\n",
    "\n",
    "    # Function to compute differences between observed and predicted NDVI and compilation in an image collection\n",
    "    def stack(year_start, year_end):\n",
    "        img_coll = ee.List([])\n",
    "        for k in range(year_start, year_end):\n",
    "            ndvi = ndvi_1yr_o.filter(ee.Filter.eq('year', k)).select('ndvi').median()\n",
    "            clim = clim_1yr_o.filter(ee.Filter.eq('year', k)).select('ndvi').median()\n",
    "            img = ndvi.addBands(clim.addBands(ee.Image(k).float())).rename(['ndvi','clim','year']).set({'year': k})\n",
    "            img_coll = img_coll.add(img)\n",
    "        return ee.ImageCollection(img_coll)\n",
    "\n",
    "    ## Function create image collection of residuals\n",
    "    def f_ndvi_clim_r_coll(year_start, year_end): \n",
    "        res_list = ee.List([])\n",
    "        #for(i = year_start i <= year_end i += 1):\n",
    "        for i in range(year_start, year_end):\n",
    "            res_image = f_ndvi_clim_r_img(i)\n",
    "            res_list = res_list.add(res_image)\n",
    "        return ee.ImageCollection(res_list)\n",
    "\n",
    "    ## Apply function to create image collection of ndvi and climate\n",
    "    ndvi_1yr_coll = f_img_coll(ndvi_1yr)\n",
    "    \n",
    "    ## Compute linear trend function to predict ndvi based on climate (independent are followed by dependent var\n",
    "    lf_clim_ndvi = ndvi_1yr_coll.select(['clim', 'ndvi']).reduce(ee.Reducer.linearFit())\n",
    "\n",
    "    ## Apply function to  predict NDVI based on climate\n",
    "    ndvi_1yr_p = ee.ImageCollection(ee.List(ndvi_1yr_coll.select('clim').iterate(f_ndvi_clim_p, first)))\n",
    "\n",
    "    ## Apply function to compute NDVI annual residuals\n",
    "    ndvi_1yr_r  = f_ndvi_clim_r_coll(year_start,year_end)\n",
    "\n",
    "    print(ndvi_1yr_r)\n",
    "\n",
    "    ## Fit a linear regression to the NDVI residuals\n",
    "    lf_trend = ndvi_1yr_r.select(['year', 'ndvi_res']).reduce(ee.Reducer.linearFit())\n",
    "\n",
    "    ## Compute Kendall statistics\n",
    "    mk_trend  = stats.mann_kendall(ndvi_1yr_r.select('ndvi_res'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Function to create image collection with annual NDVI integrals and years\n",
    "\"\"\"\n",
    "if (method == 'rue_trend' or method == 'wue_trend' ):\n",
    "    def f_img_coll(ndvi_stack):\n",
    "        img_coll = ee.List([])\n",
    "        for (k = year_start k <= year_end  k += 1):\n",
    "            ndvi_img = ndvi_stack.select('y'+k).divide(climate_1yr.select('y'+k))\n",
    "                                .addBands(ee.Image(k).float())\n",
    "                                .rename(['rue','year']).set({'year': k})\n",
    "        img_coll = img_coll.add(ndvi_img)\n",
    "    return ee.ImageCollection(img_coll)\n",
    "\"\"\"\n",
    "if (method == 'rue_trend' or method == 'wue_trend' ):\n",
    "    def f_img_coll(ndvi_stack):\n",
    "        for k in range(year_start, year_end):\n",
    "            ndvi = ndvi_1yr.filter(ee.Filter.eq('year', k)).select('ndvi').median()\n",
    "            clim = climate_1yr.filter(ee.Filter.eq('year', k)).select('ndvi').median()\n",
    "            img = ndvi.addBands(clim.addBands(ee.Image(k).float())).rename(['ndvi','clim','year']).set({'year': k})\n",
    "            img_coll = img_coll.add(img)\n",
    "   \n",
    "    ## Apply function to compute rue and store as a collection\n",
    "    rue_1yr_coll = f_img_coll(ndvi_1yr)\n",
    "\n",
    "    ## Compute linear trend function to predict ndvi based on year (ndvi trend)\n",
    "    lf_trend = rue_1yr_coll.select(['year', 'rue']).reduce(ee.Reducer.linearFit())\n",
    "\n",
    "    ## Compute Kendall statistics\n",
    "    mk_trend  = stats.mann_kendall(rue_1yr_coll.select('rue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Create 2 band output raster\n",
    "slope = lf_trend.select('scale')\n",
    "attri = ee.Image(0).where(lf_trend.select('scale').gt(0)and(mk_trend.abs().gte(kendall)),  1)\\\n",
    ".where(lf_trend.select('scale').lt(0)and(mk_trend.abs().gte(kendall)), -1)\\\n",
    ".where(mk_trend.abs().lte(kendall), 0)\\\n",
    ".where(landc.eq(210),2)\\\n",
    ".where(landc.eq(190),3)\n",
    "                       \n",
    "output = slope.addBands(attri).rename(['slope','attri'])\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "export = {\n",
    "    \"image\": output,\n",
    "    \"description\": \"ken_modis_ndvitrend\",\n",
    "    \"bucket\": \"ldmt\",\n",
    "    \"region\": util.get_coords(json.loads(util.sen_geojson)),\n",
    "    \"maxPixels\": 100000000000,\n",
    "    \"scale\": pixel}\n",
    "\n",
    "task = ee.batch.Export.image.toCloudStorage(**export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EXECUTION_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-22d236762dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m export = {\n\u001b[1;32m      2\u001b[0m         \u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlf_trend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scale'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmk_trend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkendall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m99999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlf_trend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scale'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m99999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m99999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[1;34m'description'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mEXECUTION_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[1;34m'fileNamePrefix'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mEXECUTION_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[1;34m'bucket'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBUCKET\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EXECUTION_ID' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "export = {\n",
    "        'image': lf_trend.select('scale').where(mk_trend.abs().lte(kendall), -99999).where(lf_trend.select('scale').abs().lte(0.000001), -99999).unmask(-99999),\n",
    "        'description': EXECUTION_ID,\n",
    "        'fileNamePrefix': EXECUTION_ID,\n",
    "        'bucket': BUCKET,\n",
    "        'maxPixels': 10000000000,\n",
    "        'scale': 250,\n",
    "        'region': util.sen_geojson\n",
    "    }\n",
    "\n",
    "logger.debug(\"Setting up task.\")\n",
    "task = ee.batch.Export.image.toCloudStorage(**export)\n",
    "\n",
    "logger.debug(\"Starting task.\")\n",
    "task.start()\n",
    "task_state = task.status().get('state')\n",
    "while task_state == 'READY' or task_state == 'RUNNING':\n",
    "    task_progress = task.status().get('progress', 0.0)\n",
    "    # update GEF-EXECUTION progress\n",
    "    logger.send_progress(task_progress)\n",
    "    logger.debug(\"Task progress {}.\".format(task_progress))\n",
    "    # update variable to check the condition\n",
    "    task_state = task.status().get('state')\n",
    "    sleep(5)\n",
    "\n",
    "logger.debug(\"Leaving integral_trends function.\")\n",
    "return \"https://{}.storage.googleapis.com/{}.tif\".format(BUCKET, EXECUTION_ID)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-135-f52eb156afb5>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-135-f52eb156afb5>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def run(params, logger):\n",
    "    \"\"\".\"\"\"\n",
    "    logger.debug(\"Loading parameters.\")\n",
    "\n",
    "    year_start = params.get('year_start', 2003)\n",
    "    year_end = params.get('year_end', 2015)\n",
    "    geojson = json.loads(params.get('geojson', util.sen_geojson))\n",
    "    resolution = params.get('resolution', 250)\n",
    "    dataset = params.get('dataset', 'AVHRR')\n",
    "\n",
    "    # Check the ENV. Are we running this locally or in prod?\n",
    "    if params.get('ENV') == 'dev':\n",
    "        EXECUTION_ID = str(random.randint(1000000, 99999999))\n",
    "    else:\n",
    "        EXECUTION_ID = params.get('EXECUTION_ID', None)\n",
    "\n",
    "    logger.debug(\"Running main script.\")\n",
    "    url = integral_trend(year_start, year_end, geojson, resolution, dataset, \n",
    "            EXECUTION_ID, logger)\n",
    "\n",
    "    logger.debug(\"Setting up results JSON.\")\n",
    "    results_url = CloudUrl(url, 'TODO_HASH_GOES_HERE') \n",
    "    cloud_dataset = CloudDataset('geotiff', 'integral_trends', results_url)\n",
    "    gee_results = GEEResults('cloud_dataset', cloud_dataset)\n",
    "    results_schema = GEEResultsSchema()\n",
    "    json_result = results_schema.dump(gee_results)\n",
    "\n",
    "    logger.debug(\"Leaving run function.\")\n",
    "    return json_result.data\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
